[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "welcome!"
  },
  {
    "objectID": "Tip/2023-02-20-tips.html",
    "href": "Tip/2023-02-20-tips.html",
    "title": "download files from Github",
    "section": "",
    "text": "- Github에 주피터노트북 파일 다운로드 받는 법\n\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/STGCN/2022-12-29-STGCN-tutorial.ipynb\n\n--2023-02-20 22:35:38--  https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/STGCN/2022-12-29-STGCN-tutorial.ipynb\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1184264 (1.1M) [text/plain]\nSaving to: ‘2022-12-29-STGCN-tutorial.ipynb’\n\n2022-12-29-STGCN-tu 100%[===================>]   1.13M  --.-KB/s    in 0.03s   \n\n2023-02-20 22:35:39 (42.5 MB/s) - ‘2022-12-29-STGCN-tutorial.ipynb’ saved [1184264/1184264]\n\n\n\n좌측 사이드바를 확인해보면 원하는 주피터 파일 (2022-12-29-STGCN-tutorial.ipynb) 이 잘 다운받아진 것을 확인해볼 수 있다."
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n<torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7f89e2b6e340>\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#learn",
    "title": "STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:08<00:00,  1.38s/it]"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/STGCN/튜토리얼/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html",
    "href": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html",
    "title": "Toy Example",
    "section": "",
    "text": "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#data",
    "href": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#data",
    "title": "Toy Example",
    "section": "data",
    "text": "data\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "href": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "title": "Toy Example",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#learn",
    "href": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#learn",
    "title": "Toy Example",
    "section": "Learn",
    "text": "Learn\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [03:48<00:00,  4.57s/it]"
  },
  {
    "objectID": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "href": "posts/STGCN/튜토리얼/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "title": "Toy Example",
    "section": "예제의 차원 조사",
    "text": "예제의 차원 조사\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\n1068: number of nodes // 1068개의 노드가 있음\n14: number of features // 하나의 노드에 맵핑된 차원의수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n1068: number of nodes\n\n\n_x.shape\n\ntorch.Size([1068, 14])"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html",
    "href": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html",
    "title": "튜토리얼 따라가기2",
    "section": "",
    "text": "Toy Example"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#data",
    "href": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#data",
    "title": "튜토리얼 따라가기2",
    "section": "Data",
    "text": "Data\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#recurrentgcn",
    "href": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#recurrentgcn",
    "title": "튜토리얼 따라가기2",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n        \n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#learn",
    "href": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#learn",
    "title": "튜토리얼 따라가기2",
    "section": "Learn",
    "text": "Learn\n\n# 오래걸림 주의\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 14, filters = 32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat - snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:21<00:00,  7.63s/it]"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#예제의-차원-조사",
    "href": "posts/STGCN/STGCN 공부/2023-02-23-stgcn-tutorial2.html#예제의-차원-조사",
    "title": "튜토리얼 따라가기2",
    "section": "예제의 차원 조사",
    "text": "예제의 차원 조사\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x\n    _edge_index = snapshot.edge_index\n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\n\n1068: number of nodes # 1068개의 노드가 있음\n14: number of features # 하나의 노드에 맵핑된 차원의 수\n\n\n_edge_index.shape\n\n\n_edge_attr.shape\n\n\n_y.shape\n\n\n1068: number of nodes\n\n\n_x.shape"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/traffic_prediction.html",
    "href": "posts/STGCN/STGCN 공부/traffic_prediction.html",
    "title": "Traffic Forecasting review",
    "section": "",
    "text": "https://www.youtube.com/watch?v=Rws9mf1aWUs\n\n\n\n\nimport torch\nfrom IPython.display import clear_output\npt_version = torch.__version__\nprint(pt_version)\n\n1.11.0+cu113\n\n\nThis took some time for me, so be patient :)\n\n!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${pt_version}.html\n!pip install torch-geometric\n!pip install torch-geometric-temporal\nclear_output()\n\n\n\n\n\nTraffic forecasting dataset based on Los Angeles Metropolitan traffic\n207 loop detectors on highways\nMarch 2012 - June 2012\nFrom the paper: Diffusion Convolutional Recurrent Neural Network\n\n\nimport numpy as np\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import StaticGraphTemporalSignal\n\nloader = METRLADatasetLoader()\ndataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n\nprint(\"Dataset type:  \", dataset)\nprint(\"Number of samples / sequences: \",  len(set(dataset)))\n\nDataset type:   <torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal object at 0x7f455e5315d0>\nNumber of samples / sequences:  34249\n\n\n\n\n\n207 nodes\n2 features per node (speed, time)\n12 timesteps per bucket (12 x 5 min = 60 min)\nLabels for 12 future timesteps (normalized speed) –> node regression\nEdge_attr is build based on the distances between sensors + threshold\nFurther details: https://pytorch-geometric-temporal.readthedocs.io/en/latest/_modules/torch_geometric_temporal/dataset/metr_la.html#METRLADatasetLoader\nRaw data: https://graphmining.ai/temporal_datasets/METR-LA.zip\n\n\n# Show first sample\nnext(iter(dataset))\n\nData(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])\n\n\n\n# Important: It is not always like that!\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nd = ChickenpoxDatasetLoader().get_dataset(lags=4)\nnext(iter(d))\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\nYou can always have a look at the source-code to see how a dataset is constructed. Chickenpox would be a classical “predict-next-timestep” dataset (the label is one step later than the features).\nMETR-LA would be a sequence-to-sequence prediction dataset that predicts further into the future than just the next timestep. You can also see, that the features are used as label as well.\n# >>> From the ChickenpoxDatasetLoader <<<\nself.features = [\n            stacked_target[i : i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ]\nself.targets = [\n            stacked_target[i + self.lags, :].T  \n            for i in range(stacked_target.shape[0] - self.lags)\n        ]\n\n# >>> From METRLADatasetLoader <<<\nindices = [\n            (i, i + (num_timesteps_in + num_timesteps_out))\n            for i in range(self.X.shape[2] - (num_timesteps_in + num_timesteps_out) + 1)\n        ]\nfor i, j in indices:\n            features.append((self.X[:, :, i : i + num_timesteps_in]).numpy())\n            target.append((self.X[:, 0, i + num_timesteps_in : j]).numpy())\n\nimport seaborn as sns\n# Visualize traffic over time\nsensor_number = 1\nhours = 24\nsensor_labels = [bucket.y[sensor_number][0].item() for bucket in list(dataset)[:hours]]\nsns.lineplot(data=sensor_labels)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f455d15d310>\n\n\n\n\n\n\n\n\n\nfrom torch_geometric_temporal.signal import temporal_signal_split\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\nprint(\"Number of train buckets: \", len(set(train_dataset)))\nprint(\"Number of test buckets: \", len(set(test_dataset)))\n\nNumber of train buckets:  27399\nNumber of test buckets:  6850\n\n\n\n\n\n\nWhich model to choose depends on which time-series task you work on.\n\nA3TGCN is an extension of TGCN that uses attention\nThe spatial aggregation uses GCN, the temporal aggregation a GRU\nWe can pass in periods to get an embedding for several timesteps\nThis embedding can be used to predict several steps into the future = output dimension\nWe could also do this in a loop and feed it again into the model (would be autoregressive)\nThere is only one block here. Other layers also allow stacking???\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import A3TGCN\n\nclass TemporalGNN(torch.nn.Module):\n    def __init__(self, node_features, periods):\n        super(TemporalGNN, self).__init__()\n        # Attention Temporal Graph Convolutional Cell\n        self.tgnn = A3TGCN(in_channels=node_features, \n                           out_channels=32, \n                           periods=periods)\n        # Equals single-shot prediction\n        self.linear = torch.nn.Linear(32, periods)\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        x = Node features for T time steps\n        edge_index = Graph edge indices\n        \"\"\"\n        h = self.tgnn(x, edge_index)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nTemporalGNN(node_features=2, periods=12)\n\nTemporalGNN(\n  (tgnn): A3TGCN(\n    (_base_tgcn): TGCN(\n      (conv_z): GCNConv(2, 32)\n      (linear_z): Linear(in_features=64, out_features=32, bias=True)\n      (conv_r): GCNConv(2, 32)\n      (linear_r): Linear(in_features=64, out_features=32, bias=True)\n      (conv_h): GCNConv(2, 32)\n      (linear_h): Linear(in_features=64, out_features=32, bias=True)\n    )\n  )\n  (linear): Linear(in_features=32, out_features=12, bias=True)\n)\n\n\n\n\n\n\nTraining on GPU didn’t bring much speed-up\nI ran into RAM issues, why I only train on a smaller subset of the data\n\n\n# GPU support\ndevice = torch.device('cpu') # cuda\nsubset = 2000\n\n# Create model and optimizers\nmodel = TemporalGNN(node_features=2, periods=12).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nprint(\"Running training...\")\nfor epoch in range(10): \n    loss = 0\n    step = 0\n    for snapshot in train_dataset:\n        snapshot = snapshot.to(device)\n        # Get model predictions\n        y_hat = model(snapshot.x, snapshot.edge_index)\n        # Mean squared error\n        loss = loss + torch.mean((y_hat-snapshot.y)**2) \n        step += 1\n        if step > subset:\n          break\n\n    loss = loss / (step + 1)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(\"Epoch {} train MSE: {:.4f}\".format(epoch, loss.item()))\n\nRunning training...\n\n\n\n\n\n\nLets get some sample predictions for a specific horizon (e.g. 288/12 = 24 hours)\nThe model always gets one hour and needs to predict the next hour\n\n\nmodel.eval()\nloss = 0\nstep = 0\nhorizon = 288\n\n# Store for analysis\npredictions = []\nlabels = []\n\nfor snapshot in test_dataset:\n    snapshot = snapshot.to(device)\n    # Get predictions\n    y_hat = model(snapshot.x, snapshot.edge_index)\n    # Mean squared error\n    loss = loss + torch.mean((y_hat-snapshot.y)**2)\n    # Store for analysis below\n    labels.append(snapshot.y)\n    predictions.append(y_hat)\n    step += 1\n    if step > horizon:\n          break\n\nloss = loss / (step+1)\nloss = loss.item()\nprint(\"Test MSE: {:.4f}\".format(loss))\n\n\n\n\nThe further away the point in time is, the worse the predictions get\nPredictions shape: [num_data_points, num_sensors, num_timesteps]\n\n\nimport numpy as np\n\nsensor = 123\ntimestep = 11 \npreds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in predictions])\nlabs  = np.asarray([label[sensor][timestep].cpu().numpy() for label in labels])\nprint(\"Data points:,\", preds.shape)\n\nData points:, (289,)\n\n\n\nimport matplotlib.pyplot as plt \nplt.figure(figsize=(20,5))\nsns.lineplot(data=preds, label=\"pred\")\nsns.lineplot(data=labs, label=\"true\")\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7b14858950>"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html",
    "title": "튜토리얼 따라가기1",
    "section": "",
    "text": "https://miruetoto.github.io/yechan3/posts/3_Researches/STGCN/2022-12-29-STGCN-tutorial.html\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#imports",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#imports",
    "title": "튜토리얼 따라가기1",
    "section": "imports",
    "text": "imports\n\n# 일반적인 모듈\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx # 그래프 시그널 시각화를 위한 모듈\nfrom tqdm import tqdm # for문의 진행 상태 확인\n\n# 파이토치 관련\nimport torch\nimport torch.nn.functional as F\n\n\n# PyG 관련\nfrom torch_geometric.data import Data # 그래프 자료를 만들기 위한 클래스\n\n\n# STGCN 관련\nimport torch_geometric_temporal\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\nfrom torch_geometric_temporal.signal import temporal_signal_split # STGCN dataset을 train/test set으로 분리\n\n- STGCN의 학습을 위한 클래스 선언\n\n# define recurrent GCN architecture\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#notations-of-stgcn",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#notations-of-stgcn",
    "title": "튜토리얼 따라가기1",
    "section": "notations of STGCN",
    "text": "notations of STGCN\n- 시계열: each \\(t\\) 에에 대한 observation이 하나의 값 (혹은 벡터)\n\n자료: \\(X(t) \\quad \\text{for} \\quad t = 1,2,\\dots,T\\)\n\n- STGCN setting에서는 each \\(t\\) 에 대한 observation이 graph\n\n자료: \\(X(v,t) \\quad \\text{for} \\quad t = 1,2,\\dots,T \\quad \\text{and} \\quad v\\in V\\)\n주의: 이 포스트에서는 \\(X(v,t)\\)를 \\(f(v,t)\\)로 표현할 때가 있음"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#dataset-dataloaders",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#dataset-dataloaders",
    "title": "튜토리얼 따라가기1",
    "section": "dataset, dataloaders",
    "text": "dataset, dataloaders\n\nPyG의 Data 자료형\n(예제) 아래와 같은 그래프 자료를 고려하자.\nWe show a simple example of an unweighted and undirected graph with three nodes and four edges. Each node contains exactly one feature\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype = torch.long)  # 4 edges\nx  = torch.tensor([[-1], [0], [1]], dtype = torch.float) # 3 nodes\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nx : \\(3\\times1\\) 크기의 행렬 \\(\\to\\) 3개의 노드와 각 노드는 단일 값을 가진다.\nedge_index : \\(2 \\times 4\\) 크기의 행렬 \\(\\to\\) \\(4\\)개의 엣지들 (양방향 그래프)\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x # 노드의 특징 행렬\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index # 그래프 연결성\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])\n\n\n\ndata.num_edges # edge 총 갯수\n\n4\n\n\n\ndata.is_directed() # 그래프 방향성 여부 확인\n\nFalse"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#pytorch-geometric-temporal-의-자료형",
    "title": "튜토리얼 따라가기1",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geomatric Temporal Signal\n\n아래의 클래스들 중 하나를 이용하여 만든다.\n\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n\ntorch_geometric_temporal.signal.dynamic_hetero_graph_static_signal.DynamicHeteroGraphStaticSignal\n\n\n이 중 “Heterogeneous Temporal Signal”은 우리가 관심이 있는 신호가 아님로 사실상 아래 3가지만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\) 와 같은 구조를 의미한다.\n\n(예제1) StaticGraphTemporalSignal을 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\nlen(data_dict['node_ids']) # node 개수는 20개\n\n20\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nlen(data_dict['edges']) # edge의 개수 102개\n\n102\n\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉, data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict['edges']).T\nedge_weight = np.ones(edges.shape[1])\nf =  np.array(data_dict['FX'])\n\n\n여기에서 edges는 \\(\\cal{E}\\) 에 대한 정보들\nedges_weight는 \\(\\bf{W}\\)에 대한 정보들\nf는 \\(\\bf{f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\(\\bf{W} = \\bf{E}\\)로 정의한다.\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 이다.\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index = edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets)\n\n\ndataset\n\n<torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7fc4bee0a250>\n\n\n- 그런데 이 과정을 아래와 같이 할 수도있음\n\n# Pytorch Geometric Temporal 공식홈페이지에 소개된 콛,ㅡ\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags = 4)\n\n- dataset은 dataset[0],\\(\\dots\\)dataset[516]과 같은 방식으로 각 시점별 자료에 접근 가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x\n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([\\bf{f}_1, \\bf{f}_2, \\bf{f}_3, \\bf{f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]이 값들과 같음. 즉 \\(\\bf{f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#summary-of-data",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#summary-of-data",
    "title": "튜토리얼 따라가기1",
    "section": "Summary of data",
    "text": "Summary of data\n\n\\(T = 519\\)\n\\(N=20\\) # number of nodes\n\\(|\\cal{E}|=102\\) # edges\n\\(f(t,v)\\)의 차원? \\((1,)\\) # edges\n시간에 따라서 Number of nods가 변하는지? \\(\\to\\) False\n\\(\\bf{X}: (20, 4)\\)\n\\(\\bf{y}: (20, )\\)\n예제코드적용가능 여부 : Yes\n\n- Nodes : 20 - vertices are counties\n- Edges: 102 - edges are neighbourhoods\n- Time: 519 - between 2004 and 2014 - per weeks\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags = 4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio = 0.8)"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#learn",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#learn",
    "title": "튜토리얼 따라가기1",
    "section": "Learn",
    "text": "Learn\n\nmodel = RecurrentGCN(node_features = 4, filters = 32)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat - snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:08<00:00,  1.37s/it]"
  },
  {
    "objectID": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#visualization",
    "href": "posts/STGCN/STGCN 공부/2023-02-21-SGGCN-tutorial1.html#visualization",
    "title": "튜토리얼 따라가기1",
    "section": "Visualization",
    "text": "Visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\nV\n\n['BACS',\n 'BARANYA',\n 'BEKES',\n 'BORSOD',\n 'BUDAPEST',\n 'CSONGRAD',\n 'FEJER',\n 'GYOR',\n 'HAJDU',\n 'HEVES',\n 'JASZ',\n 'KOMAROM',\n 'NOGRAD',\n 'PEST',\n 'SOMOGY',\n 'SZABOLCS',\n 'TOLNA',\n 'VAS',\n 'VESZPREM',\n 'ZALA']\n\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-02-21-gcn1.html",
    "href": "posts/GCN/2023-02-21-gcn1.html",
    "title": "Graph Convolutional Network",
    "section": "",
    "text": "GCN의 개념에 대해 학습해보자.(40:39)\n\n\n\n\n\nMany important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few).\n\n대부분의 머신러닝 알고리즘은 입력 데이터가 유클리디안 공간 (Euclidean space)에 존재함을 가정하고 있다. 즉, 통계 데이터나 이미지처럼 입력 데이터가 벡터의 형태로 표현될 수 있어야 한다. 그러나 소셜 네트워크, 관계형 데이터베이스, 분자 구조 등과 같이 객체들과 그 객체들 간의 관계로 표현되는 데이터는 기본적으로 위와 같은 그래프로 표현된다.\n또한, 만약 사용자나 원자의 속성, 연결의 종류 등을 고려해야하는 경우에는 단순히 node와 edge로 이루어진 그래프가 아니라, node feature matrix와 edge feature matrix가 추가된 속성 그래프 (attributed graph)로 데이터를 표현해야 한다. 이러한 형태의 그래프 데이터는 유클리드 공간에 존재하지 않으며, 직접적인 방식으로 벡터의 형태로 변환하는 것 또한 불가능하다. 따라서, 벡터 형태의 입력 데이터를 가정하는 기존의 인공신경망 (Artificial Neural Network, ANN)으로는 분자 구조와 같은 그래프 형태의 데이터를 처리할 수 없다는 문제점이 존재한다.\n(+) 행동 인식 분야에서 가장 핫하게 등장하는 네트워크가 바로 GCN(Graph Convolutional Networks)이다. GCN은 쉽게 설명하자면, 어떤 그래프 구조를 이미지 convolution과 유사한 방식으로 연산해서 특징점을 추출하는 네트워크라고 보면 될 것 같다. 사람의 몸도 어떻게 보면 각 관절과 그 관절들이 연결되어있는 구조로 그래프 구조라고 볼 수 있다. 그렇기 때문에 GCN을 사용한 논문들에서도 좋은 성능을 보이고 있다.\n\nref: (AAAI-2018) Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\n\n\n\n\n\n\nGraph는 vertex(node)와 edge로 이루어져있다. 이 때 node는 한 input data를 의미하고 edge는 두 데이터 간의 relationship을 의미한다. (어차피 같은 의미이지만 앞으로는 vertex 대신 node라는 표현을 많이 사용할 것이다.)\n\n소셜 그래프에서 node는 사람, edge는 두 사람 사이의 관계를 의미한다.\nWeighted Grapgh vs. Unweighted Graph\nDirected Graph vs. Undirected Graph\n(참고) 위의 그래프의 경우 방향성이 존재하지 않는 undirected Graph이다.\n\n\n\n\n모든 노드간의 relationship 정보를 담고있도록 data를 표현해야하므로 이 정보들은 1. Adgacency matrix로 나타낼 수 있다. 또한 노드간의 relationship정보 말고도 node 자체가 가지고 있는 feature 정보가 있으므로 이는 2. Feature matrix로 나타낸다.\n\n1 Network(Graph data) \\(\\to\\) Adjacency matrix\n\n\\(n\\) 개의 노드가 있다면 Adjacency matrix는 \\(n\\times n\\) 크기를 갖게 된다.\n\n5개의 노드가 있으므로 \\(5\\times 5\\) Adjacency matrix\n\n\\(\\bf{A}_{ij}\\) : Adjacency matrix의 \\(i\\)번째 row와 \\(j\\)번째 컬럼에 있는 값을 나타내며, \\(i\\)번째 노드와 \\(j\\)번째 노드가 서로 연결이 되어 있는지를 나타낸다.\n\n노드 사이에 엣지가 있는지? (있으면 \\(1\\), 없으면 \\(0\\))\n\n\n2 Feature matrix\n\nFeature matirx로 각 노드의 정보를 나타낸다.\n\nFeature matrix의 크기는 \\(n(\\text{노드의 수}) \\times f(\\text{feature 개수})\\) 이며, \\(f\\)는 설정함에 따라서 많아질 수도 있고 적어질 수도 있는 값이다.\nfeature matrix를 \\(\\bf{X}\\) 라고 하자, 이 때 \\(\\bf{X}_{ij}\\) 가 의미하는 것은 i번째 노드에 j번째 feature가 무엇인지 나타내는 것이다.\n\n\n\n\n\n\n\n데이터의 구조를 고려해야 한다는 점은 이미지뿐만 아니라 그래프 데이터에서도 매우 중요하기 때문에 이미지에 대한 convolution을 일반화하여 그래프 데이터에 적용하기 위한 연구가 머신러닝 분야에서 활발히 진행되었다. 그래프 합성곱 신경망 (Graph Convolutional Network, GCN)은 이미지에 대한 convolution을 그래프 데이터로 확장한 머신러닝 알고리즘이다.\n이부분에 대해 이해를 하려면 먼저 CNN에 이해가 필요할 것 같다.\n\n\n\n\n\nStanford, cs231n 2017\n\n\n\n\n\nReduce the number of parameters \\(\\to\\) less overfitting, low computational cost\nLearn local features\nTranslation invariance\n\n\n\n\n\n어쨌든 얘도 convolutional network니까 CNN을 보고 이것을 이미지가 아닌 그래프에 적용시켜본다면 구조를 어떻게 바꿔야 할지지 생각해보자.\n\nCNN updates values in activation map in each layer. Values of activation map determine the state of image.\nValues of each node feature determine the state of graph. \\(\\to\\) Make each layer of network update values of each node feature\n\n그래프의 정보를 결정하는 것은 무엇일까? 이미지는 activation map에 있는 value들이 그 이미지에 담긴 상태가 뭔지, 이미지에 담긴 정보가 뭔지를 결정을 하는 값들이었는데 그래프 같은 경우에는 각 노드에 담긴 value 즉, node feature matrix 안에 담긴 정보가 업데이트 되도록 하면 되겠다.\n결국 중요한 것은 Graph Convolutional Layer를 거치게 되면 노드 피처에 담긴 값이 업데이트가 되어야 한다는 것을 convolutional network를 통해 알 수 있었다.\n그렇다면 어떤 방식으로 업데이트를 해야 타당할까?\n어쨌든 이것도 컨볼루션이니까 컨볼루션은 어떤 작은 웨이트를 쭉 이동시키는 연산이었는데 중요한 특성은 Weight sharing을 한다는 것이었고, 어떤 로컬한 이 값 근처에 있는 값들만 weight에 들어가서 로컬한 피처를 배운다는 것이 또 하나의 특징이었다. 그래서 그 뉴런이 receptive field를 갖게 된다. (어떤 전체의 데이터에 정보를 하나의 뉴런이 다 받는게 아니라 어떤 로컬한 부분에 있는 정보를 이제 뉴런이 받게 되고 이를 receptive field 라고 한다.)\n그런 특성을 그럼 그래프에는 어떻게 적용시켜야 될까? 노드의 피처를 계쏙 업데이트 한다는 것은 각 노드의 정보를 업데이트 하는 것 그래서 노드 피처 매트릭스를 다시 그려보면 \\(n\\)개의 노드가 있는 그래프일 때 \\(n\\times f\\text{(feature 개수)}\\)의 shape을 갖고 이 matrix의 i번째 row가 의미하는 것은 i번째 노드의 피처/상태/정보를 담고있다고 앞에서 배웠다.\n아까 말했듯이 layer를 하나 거쳐서 이 node feature matrix를 업데이트 한다는 것은 이 각각의 row를 즉, 각각의 노드의 정보를 업데이트 해주면 될 것 같다.\n그럼 어떤 방식으로 업데이트를 할 거냐? convolution network 같이 그 주변에 있는 애들의 정보만 받아서 업데이트를 하자 이런식으로 생각해 볼 수 있을 것 같다.\n\n\n\n\n\n\nimage.png\n\n\n\\[H_1^{(l+1)} = \\sigma(H_1^{(l)}W^{(l)} + H_2^{(l)}W^{(l)} + H_3^{(l)}W^{(l)} + H_4^{(l)}W^{(l)} + b^{(l)})\\]\n\\[\\Rightarrow H_i^{(l+1)} = \\sigma\\Big(\\sum_{j\\in N(i)} H_j^{(l)}W^{(l)} + b^{(l)} \\Big)\\]\n\\[W: \\text{weight},\\quad W^{(l)}: l\\text{번째 layer의 weight},\\quad H:\\text{hidden state}, \\quad \\sigma: \\text{activation function}\\]\n(참고) 여기서는 node feature matrix를 hidden state라고 부를 것임\n위의 그림과 같은 그래프가 있다고 가정해보자. 그래프의 번호가 1번, 2번, 3번, 4번 이런식으로 붙여졌다고 할 때 \\(H_1^{(l+1)}\\), 즉 하나의 \\((l+1)\\)번 째 layer를 통과하게 되면 \\(H_1\\)의 정보는 자기 자신의 웨이트를 더하고, 그 다음에 연결되어 있는 \\(H_2\\)의 hidden state에 weight를 곱하고, \\(H_3\\)의 hidden state에 weight를 곱하고 \\(H_4\\)의 hidden state에 weight를 곱하고 bias를 더해서 activation을 거쳐서 다음 layer의 값을 업데이트 하면 되겠다.\n이렇게 되면 이제 convolutional layer처럼 어떤 local한 feature를 뽑아낼 수 있고, 그럼 이 다음 노드가 받은 것은 1번, 2번, 3번, 4번 노드의 정보만 받아서 다음 뉴런에 전달을 해주는 것이고, 연결되지 않은 5번, 6번, 7번에 대해서는 들어가지 않았으니까 1번 노드의 정보는 1번 노드의 근처에 있는 로컬한 정보를 뽑아냈다라고 볼 수 있다.\n또한 이 weight가 다 똑같기 때문에 weight sharing을 한다. 즉, 전체가 다 연결되어 있는게 아니라 어쨌든 얘도 어떤 노드의 정보는 그 구조가 다 비슷할 것이다. 왜냐하면 처음에 같은 이 feature의 순서가 똑같았으니까 거기에 어차피 얘들도 다 비슷한 애들이니까 같은 weight를 곱해서 general한 정보를 뽑아낼 수 있게 마치 LSTM에서 각각의 워드에 다 똑같은 weight를 곱해줬던 것 처럼(왜냐하면 이 word는 다 비슷한 특성을 가지고 있기 때문에) 얘들도 각각의 노드에 피처가 비슷한 성격을 띄고 있을 거니까 같은 weight를 sharing해서 곱해줘서 computational cost도 낮추고 efficiency도 높일 수 있겠다.\n그래서 이런식으로 업데이트 하면 아까 convolutional layer의 두 가지 특성이였던 weight sharing과 local feature를 뽑아낸다는 것 둘 다 가지고 있게 됐다.\n실질적으로 구현할 때 1번 노드에 연결되었는지 다 보고, 1번노드와 연결되어 있는 애들을 weight 타고 다 더한다음에 2번노드로 가서 2번노드에 뭐가 연결되어있는지 다 보고, 그 다음에 종합해서 업데이트 하고, 3번노드 뭐랑 연결되어있는지 다 보고,,이렇게 할 수는 없겠죠.(for문을 엄청나게 많이 사용해서 속도가 느려질 것이다.)\n그러면 우리가 graph structure를 adjacency matrix로 나타내는데 그럼 이 adjacency matrix가 결국은 connectivity를 담고있는 매트릭스이다. 그럼 이것을 어떻게 잘 활용하면 한번에 행렬연산으로 할 수 있을 것 같다. (행렬 연산은 gpu가 빠르게 잘함)\n- 예제\n자기자신과 연결되어있다고 가정, feature의 개수도 임의로 10으로 지정\n보기 쉽게 node feature matrix를 H라고 놓자. (오른쪽 matrix가 H임)\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n그 새로운 값은 각각의 노드의 근처에 있는 값들만 받아들여서 convolution 연산을 한 효과를 냈고, 그리고 그 weight들은 다 똑같다. 그 weight를 곱할때 filter마다는 다르지만 동인한 하나의 필터 내에서는 같은 weight들이 서로 다른 노드에 곱해진다.\n그래서 이런식으로 하면 weight sharing을 하고, local feature를 뽑아내는 convolutional layer의 특성을 가지면서도 for문을 돌고 하는게 아니라 행렬연산을 통해 이것을 구현 함으로써 GPU에 넣었을 때 훨씬 빠르고 gradient 계산하는 것도 병렬화되서 훨씬 빠르게 할 수 있기 떄문에 이렇게 구현하면 쓸 수 있겠다고 생각해볼 수 있다.\n타당한 structure인 것 같다.\n\\[H^{(l+1)} = \\sigma(AH^{(l)}W^{(l)}+b^{(l)})\\]\n다음 layer의 hidden state는 이전 layer의 hidden stat에 weight를 곱하고 여기에 adjacency matrix를 곱하면 그 connectivity에 들어가는 정보가 들어가서 각각의 노드가 연결되어 있는 애들의 정보만 받게되고, 이것도 어쨌든 컨볼루션을 했으니까 activation을 씌워주면 update된 값을 얻을 수 있다.\n\n\n\n\nPermutation invariance는 adjency matrix의 순서가 바뀌더라도 그 output이 변하지 않는 함수의 특성을 말한다.\n\n\\[f(PAP^{T}) = f(A)\\]\n(참고) 위 식에서 \\(\\bf{A}\\)는 adjacency matrix, \\(\\bf{P}\\)는 행과 열의 순서를 바꾸는 permutation matrix이다. 위 식은 위에서 설명한 것처럼 adjacency matrix 내의 노드의 순서가 바뀌어도 함수의 결과는 바뀌지 않는다.\n\n\n\nimage.png\n\n\n그래프를 adjacency matrix와 node feature matrix로 표현을 했는데 node feature matrix에 순서가 있다. 노드의 순서가 바뀐다고 해서 그래프가 달라지지는 않는다. 노드의 배치만 바뀌었을 뿐 노드의 특성과 엣지는 다 똑같이 연결되어 있으니까 같은 그래프이다.그런데 우리가 표현하는 node feature matrix는 바뀌게 될것이다. (row의 순서가 뒤죽박죽..)근데 결국 얘네들은 같은 그래프이니까 feature를 뽑아낼 때 같은 값이 나와야 된다. (순서가 다르게되어 있다고 다른값이 나오면 안되겠죠) 따라서 이걸 하기 위해서 Readout layer를 거치게 된다.\n이 Readout layer의 역할은 permutation invariance를 준다. 즉, permutation이 어떻게 되어있든 관계없이 invariance하게 하는 역할을 수행해준다. 다양한 방법이 있지만 가장 간단한 방법은 위와 같다.\n\n\n\n\n\n\n\nimage.png\n\n\nGCN을 거친 후 마지막에 Readout layer를 통해 최종적으로 classification 혹은 value를 regression한다. CNN에서 Conv-pool layer들을 거친 후 마지막에 모든 node들 정보를 취합하기 위해 FC-layer를 거친 후 softmax를 통해 classification작업을 수행한다.\n마찬가지로 Graph Neural Network에서도 graph convolution layer들을 거친 후 MLP로 모든 node 정보를 취합하고 최종적으로 regression 혹은 classification을 위해 어떤 값을 결정짓는 작업이 필요하다. 이를 GCN에서 readout-layer라고 한다\n\n\n\nimage.png\n\n\n\n\n\nGCN을 비롯한 graph neural network (GNN)을 직접 구현하는 것은 인접 행렬과 node feature matrix를 추출하는 것부터 여러 그래프의 batch를 만드는 것 까지 많은 어려움이 따른다. PyTorch를 기준으로는 Deep Graph Library (DGL)와 PyTorch Geometric이라는 라이브러리가 GNN과 이를 이용한 딥 러닝에 관한 여러 구현을 제공하고 있다.\n\n\n\n\nhttps://tkipf.github.io/graph-convolutional-networks/\nhttps://untitledtblog.tistory.com/152"
  },
  {
    "objectID": "etc/2023-02-01-memo.html",
    "href": "etc/2023-02-01-memo.html",
    "title": "메모장",
    "section": "",
    "text": "temporal signal vs. static signals\nstatic graph vs. dynamic graph\nedge index? edge feature matrix (node feature matrix는 알겠는데ㅠㅠ엣지 매트릭스는 뭐야)\nRecurrent GNN\n\n얘가 GNN의 시초라는데?\n\nGNN, Recurrent GNN, GCN, ST-GCN 순서가 어떻게 되는지 헷갈린다..\nGRU\nspectral gcn\n\n\n\n\nimage.png\n\n\n\nGNN 정리 블로그\nML with Graphs 강의영상"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "튜토리얼 따라가기2\n\n\n\n\n\n\n\nSTGCN\n\n\nBASIC\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraffic Forecasting review\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nGraph Convolutional Network\n\n\n\n\n\n\n\nGCN\n\n\nBASIC\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n튜토리얼 따라가기1\n\n\n\n\n\n\n\nSTGCN\n\n\nBASIC\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToy Example\n\n\n\n\n\n\n\nSTGCN\n\n\nPytorch\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\nSTGCN 튜토리얼\n\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\n신록예찬, 최서연\n\n\n\n\n\n\nNo matching items"
  }
]