{
  "hash": "c92fa6d5b8cb9d331f643b970a66ceab",
  "result": {
    "markdown": "---\ntitle: \"Ridge, Lasso, Elastic Net\"\nauthor: 'jiyun Lim'\ndate: '03/01/2023'\ncategories: \n  - R\n  - ML\nformat:\n  html:\n    theme: default\n---\n\n\n\n`-` 손실함수 정의\n\n**벡터 Norm의 의미**\n$$||\\beta||_1 = \\sigma_{i=1}^n|\\beta_i|$$\n$$||\\beta||_2 = \\sqrt{\\sum_{i=1}^n \\beta_i^2}$$\n\n**penalty가 붙어있는 손실함수 형태**\n\n\n \n$$PRSS(\\beta,\\lambda;X,y) = RSS(\\beta;X,y) + \\lambda J(\\beta)\n=(y-X\\beta)^\\top (y-X\\beta) + \\lambda J(\\beta)$$\n\n`-` Lasso Regression 손실함수 - ***$L_1$ norm***\n\n$$\\underset{\\beta}{min}\\space (y-X\\beta)^\\top (y-X\\beta) + \\frac{\\lambda}{2}||\\beta||_1$$\n`-` Ridge regression 손실함수 - ***$L_2$ norm***\n\n$$\\underset{\\beta}{min}\\space (y-X\\beta)^\\top (y-X\\beta) + \\frac{\\lambda}{2}||\\beta||^2_2$$\n\n\n`-` Elastic Net\n\n라쏘와 능선회귀를 하나의 모델로 합친 모델.\n\n$$\\underset{\\beta}{min}\\space (y-X\\beta)^\\top (y-X\\beta) + \\frac{\\lambda}{2}(\\alpha ||\\beta||_1 + (1-\\alpha)||\\beta||_2^2)$$\n\n- 손실함수를 두개의 파이퍼파라미터로 정의할 수 있다. $\\to \\lambda, \\alpha$\n\n- $\\lambda$는 패널티의 가중치를 조정\n- $\\alpha$는 라쏘와 릿지의 가중치를 조정\n  - $\\alpha = 1$, Lasso 회귀\n  - $\\alpha = 0$, Ridge 회귀\n  \n  \n  \n`-` 예제\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(glmnet)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(QuickStartExample)\ndata_X <- QuickStartExample$x\ny <- QuickStartExample$y\ndata_X %>% dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100  20\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ny %>% length()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- glmnet(data_X, y, alpha = 1) \nplot(result, label = TRUE)\n```\n\n::: {.cell-output-display}\n![](2023-03-09-penaly-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n  - 그래프의 각 선들이 람다값에 따른 각 변수의 계수의 변화를 나타낸다.\n  - 위쪽 숫자 의미: 현재 람다에 대응하는 계수가 0이 아닌 변수 갯수\n  - 계수 벡터의 $L_1$이 작다는 것은 ***람다값이 큰 상황*** 에 대응된다.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glmnet(x = data_X, y = y, alpha = 1) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_X <- matrix(rnorm(60), ncol = 20)\npredict(result, newx = new_X, s= 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            s1\n[1,]  1.015300\n[2,] -2.965372\n[3,]  3.350610\n```\n:::\n:::\n\n## validation 셋을 통한 람다값 결정하기.\n- 데이터셋 나누기\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\ntrain_index <- createDataPartition(y, p =0.7, list = F)\ntrain_data_X <- data_X[train_index,]\nvalid_data_X <- data_X[-train_index,]\ntrain_y <- y[train_index]\nvalid_y <- y[-train_index]\n```\n:::\n\n\n**train과 valid에서 모델 성능 비교하기.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- glmnet(train_data_X, train_y)\npred_train <- predict(result,\n                      newx = train_data_X,\n                      s = result$lambda)\n\nresidual_set <- sweep(pred_train, 1, train_y, FUN = '-')\npreform_train <- colMeans(residual_set^2)\n\npred_valid <- predict(result,\n                      newx = valid_data_X,\n                      s = result$lambda)\nresidual_set <- sweep(pred_valid, 1, valid_y, FUN = '-')\nperform_valid <- colMeans(residual_set^2)\n```\n:::\n\n\n(참고) : `sweep()` 함수는 행렬이 주어졌을때 각 열에서 (MARGIN1) 혹은 각 행에서 (MARGIN2) 주어진 벡터를 사용하여 연산을 수행함.\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  ",
    "supporting": [
      "2023-03-09-penaly-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}